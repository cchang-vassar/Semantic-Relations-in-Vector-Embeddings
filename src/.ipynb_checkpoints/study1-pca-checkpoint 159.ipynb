{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Embeddings of Arguments and Counterarguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jlLsriNpHtsk"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import re\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "from ctypes import Union\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### OpenAI Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/site-packages (1.3.9)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/site-packages (from openai) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/site-packages (from openai) (0.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/site-packages (from openai) (2.5.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.11/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /usr/local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlLsriNpHtsk",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Class Declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum for categories\n",
    "\n",
    "class Category(Enum):\n",
    "    CULTURE = \"culture\"\n",
    "    DIGITAL_FREEDOMS = \"digital-freedoms\"\n",
    "    ECONOMY = \"economy\"\n",
    "    EDUCATION = \"education\"\n",
    "    ENVIRONMENT = \"environment\"\n",
    "    FREE_SPEECH_DEBATE = \"free-speech-debate\"\n",
    "    HEALTH = \"health\"\n",
    "    INTERNATIONAL = \"international\"\n",
    "    LAW = \"law\"\n",
    "    PHILOSOPHY = \"philosophy\"\n",
    "    POLITICS = \"politics\"\n",
    "    RELIGION = \"religion\"\n",
    "    SCIENCE = \"science\"\n",
    "    SOCIETY = \"society\"\n",
    "    SPORT = \"sport\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum for analysis types\n",
    "\n",
    "class AnalysisType(Enum):\n",
    "    TSNE = \"tsne\"\n",
    "    PCA = \"pca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum for processing unit\n",
    "\n",
    "class ProcessingUnit(Enum):\n",
    "    GLOBAL = \"global\"\n",
    "    CATEGORY = \"category\"\n",
    "    DEBATE = \"debate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Arguments from File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Debate] Arguments dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1984694253.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\" Extract arguments from category file: debate_topic.txt -> full.txt\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def debate_extract_arguments(\n",
    "    \"\"\" Extract arguments from category file: debate_topic.txt -> full.txt\n",
    "    \"\"\"\n",
    "    category: Category,\n",
    "    file_path: str,\n",
    "    start_re: str = \"# PRO\",\n",
    "    end_re: str = \"# LITERATURE\",\n",
    "    pro_point_re: str = \"# PRO\\w+-POINT\",\n",
    "    pro_counter_re: str = \"# PRO\\w+-COUNTER\",\n",
    "    con_point_re: str = \"# CON\\w+-POINT\",\n",
    "    con_counter_re: str = \"# CON\\w+-COUNTER\"\n",
    "    ) -> {}:\n",
    "    \n",
    "    # try to open file from path\n",
    "    try:\n",
    "        with open(f'../arguana-counterargs-corpus/02-extracted-arguments/training/{category.value}/{file_path}/full.txt', 'r') as file:\n",
    "            file_contents = file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path + '.txt'}\")\n",
    "        return None\n",
    "        \n",
    "    # parse file contents\n",
    "    lines: [] = re.split(r'\\n', file_contents)\n",
    "\n",
    "    # Enum for argument section\n",
    "    class ArgumentSection(Enum):\n",
    "        PRO = \"pro\"\n",
    "        CON = \"con\"\n",
    "        \n",
    "    # Enum for argument type\n",
    "    class ArgumentType(Enum):\n",
    "        POINT = \"point\"\n",
    "        COUNTER = \"counter\"\n",
    "        \n",
    "    # holds the extracted arguments for the debate topic\n",
    "    debate_arguments = {}\n",
    "    \n",
    "    # holds the argument pairs data for the debate topic\n",
    "    arguments = {\n",
    "        'pro': [],\n",
    "        'con': []\n",
    "    }\n",
    "    \n",
    "    # Start looping through lines\n",
    "    current_argument: str = \"\"\n",
    "    start: bool = False\n",
    "    current_argument_section = ArgumentSection.PRO\n",
    "    current_argument_type = ArgumentType.POINT\n",
    "    cur_pair = {}\n",
    "\n",
    "    for line in lines:\n",
    "        # skip to start line\n",
    "        if (not start):\n",
    "            if re.match(r'\\s*' + start_re, line):\n",
    "                start = True\n",
    "                continue\n",
    "            continue\n",
    "        \n",
    "        # special case when we reach # LITERATURE we append the last argument and return\n",
    "        if re.match(r'\\s*' + end_re, line):\n",
    "            _append_argument_to_cur_pair(current_argument, current_argument_type, cur_pair)\n",
    "            _append_cur_pair_to_arguments(current_argument_section, arguments, cur_pair)\n",
    "            if len(arguments['pro']) and len(arguments['con']):\n",
    "                debate_arguments[file_path] = arguments\n",
    "            return debate_arguments\n",
    "\n",
    "        # skip citations\n",
    "        if re.match(r'\\s*\\[', line):\n",
    "            continue \n",
    "\n",
    "        # Append an argument to current pair\n",
    "        def _append_argument_to_cur_pair(current_argument: str, current_argument_type: ArgumentType, cur_pair: {}):\n",
    "            if len(current_argument):\n",
    "                if current_argument_type == ArgumentType.POINT:\n",
    "                    cur_pair['point'] = current_argument\n",
    "                else:\n",
    "                    cur_pair['counter'] = current_argument\n",
    "\n",
    "        # Append current argument pair to arguments\n",
    "        def _append_cur_pair_to_arguments(current_argument_section: ArgumentSection, arguments: {}, cur_pair: {}):\n",
    "            if len(cur_pair):\n",
    "                if current_argument_section == ArgumentSection.PRO:\n",
    "                    arguments[\"pro\"].append(cur_pair)\n",
    "                else:\n",
    "                    arguments[\"con\"].append(cur_pair)\n",
    "\n",
    "        # case where we meet a pro point\n",
    "        if re.match(r'\\s*' + pro_point_re, line):\n",
    "            _append_argument_to_cur_pair(current_argument, current_argument_type, cur_pair)\n",
    "            _append_cur_pair_to_arguments(current_argument_section, arguments, cur_pair)\n",
    "            current_argument_section = ArgumentSection.PRO\n",
    "            current_argument_type = ArgumentType.POINT\n",
    "            current_argument = \"\"\n",
    "            cur_pair = {}\n",
    "            continue\n",
    "\n",
    "        # case where we meet a pro counter\n",
    "        elif re.match(r'\\s*' + pro_counter_re, line):\n",
    "            _append_argument_to_cur_pair(current_argument, current_argument_type, cur_pair)\n",
    "            current_argument_section = ArgumentSection.PRO\n",
    "            current_argument_type = ArgumentType.COUNTER\n",
    "            current_argument = \"\"\n",
    "            continue\n",
    "\n",
    "         # case where we meet a con point\n",
    "        elif re.match(r'\\s*' + con_point_re, line):\n",
    "            _append_argument_to_cur_pair(current_argument, current_argument_type, cur_pair)\n",
    "            _append_cur_pair_to_arguments(current_argument_section, arguments, cur_pair)\n",
    "            current_argument_section = ArgumentSection.CON\n",
    "            current_argument_type = ArgumentType.POINT\n",
    "            current_argument = \"\"\n",
    "            cur_pair = {}\n",
    "            continue\n",
    "            \n",
    "        # case where we meet a con counter\n",
    "        elif re.match(r'\\s*' + con_counter_re, line):\n",
    "            _append_argument_to_cur_pair(current_argument, current_argument_type, cur_pair)\n",
    "            current_argument_section = ArgumentSection.CON\n",
    "            current_argument_type = ArgumentType.COUNTER\n",
    "            current_argument = \"\"\n",
    "            continue\n",
    "        \n",
    "        # remove in-text citations\n",
    "        line = re.sub(r'\\[\\w+\\]', '', line)\n",
    "        line = re.sub(r'\\s\\s+', '', line)\n",
    "        current_argument += line.strip()\n",
    "        \n",
    "    # this should never actually be reached\n",
    "    debate_arguments[file_path] = arguments\n",
    "    return debate_arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Category] Arguments dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Extract all debates from a category: list_of_<category_path>_debates.txt -> <debate_topic>.txt \"\"\"\n",
    "\n",
    "def category_extract_arguments(category: Category) -> {}:\n",
    "    # convert category.value to path syntax\n",
    "    category_path = category.value.replace('-', '_')\n",
    "    \n",
    "    # try to open file from path\n",
    "    try:\n",
    "        with open(f'./file_paths/list_of_{category_path}_debates.txt', 'r') as file:\n",
    "            file_contents = file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {f'list_of_{category_path}_debates.txt'}\")\n",
    "        return None\n",
    "        \n",
    "    # parse file contents\n",
    "    debates: [] = re.split(r'\\n', file_contents)\n",
    "    \n",
    "    # grab arguments for each debate in the category\n",
    "    category_arguments = {}\n",
    "    for i, debate in enumerate(debates):\n",
    "        # add topic and arguments to category_arguments\n",
    "        debate_arguments = debate_extract_arguments(category, debate)\n",
    "        if debate_arguments:\n",
    "            category_arguments.update(debate_extract_arguments(category, debate))\n",
    "        else:\n",
    "            _write_invalid_debate_to_file(category, debate)\n",
    "    category_arguments = {f'{category.value}': category_arguments}\n",
    "    return category_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _write_invalid_debate_to_file(category: Category, file_path: str):\n",
    "    output_folder = f'../data_dump/data_valid_tally/'\n",
    "    output_file_path = f'{output_folder}{category.value}.txt'\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    file = open(output_file_path, \"a\")\n",
    "    file.write(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Global] Arguments dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Extract all debates across all categories: all_categories.txt -> list_of_<category>_debates.txt \"\"\"\n",
    "\n",
    "def global_extract_arguments() -> {}:\n",
    "    # open global file from path\n",
    "    with open('./file_paths/all_categories.txt', 'r') as global_file:\n",
    "        global_file_contents = global_file.read()\n",
    "        \n",
    "    # parse file contents\n",
    "    category_pattern = re.compile(r'list_of_(\\w+)_debates')\n",
    "    lines: [] = re.split(r'\\n', global_file_contents)\n",
    "    category_paths = [line for line in lines if category_pattern.search(line)]\n",
    "    category_names = [category_pattern.search(category).group(1).upper() for category in category_paths]\n",
    "    \n",
    "    # key: category: Category.value\n",
    "    # value: dictionary of dictionaries where key = topic and value is {'pro: [{'point':, 'counter':}, ...], 'con': []}\n",
    "    global_arguments = {} \n",
    "    # add valid topics as keys to extracted_categories and grab their arguments\n",
    "    for index, category_str in enumerate(zip(category_paths, category_names)):\n",
    "        try:\n",
    "            category = Category[category_str[1]]\n",
    "            global_arguments.update(category_extract_arguments(category))\n",
    "        except KeyError as e:\n",
    "            print(f\"Category: {category_str[1]}, Category not found in Category enum and is removed.\")\n",
    "            category_paths.pop(index)\n",
    "            category_names.pop(index)\n",
    "    return global_arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Convert to df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Write to File] Arguments df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Write arguments df to pickle file \"\"\"\n",
    "\n",
    "def _arguments_df_write_to_file(\n",
    "        arguments_data: pd.DataFrame,\n",
    "        category: Optional[str] = None,\n",
    "        topic: Optional[str] = None\n",
    "    ):\n",
    "\n",
    "    # Debate case\n",
    "    if topic and category:\n",
    "        topic_path = topic.replace('-', '_')\n",
    "        output_folder = f'../data_dump/arguments_dump/{category}/'\n",
    "        output_file_path = f'{output_folder}{topic_path}_arguments.pkl'\n",
    "    \n",
    "    # Category case\n",
    "    elif category:\n",
    "        output_folder = f'../data_dump/arguments_dump/{category}/'\n",
    "        output_file_path = f'{output_folder}{category}_arguments.pkl'\n",
    "    \n",
    "    # Global case\n",
    "    else:\n",
    "        output_folder = f'../data_dump/arguments_dump/'\n",
    "        output_file_path = f'{output_folder}global_arguments.pkl'\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    arguments_data.to_pickle(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Debate] Arguments df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Convert arguments dict into df \"\"\"\n",
    "\n",
    "def debate_convert_to_df(debate_arguments: {}, category: str) -> pd.DataFrame:\n",
    "    debate_arguments_df = pd.DataFrame()\n",
    "    debate_topic = next(iter(debate_arguments))\n",
    "\n",
    "    # loop through all argument pairs in the # PRO section\n",
    "    for i, pro_argument in enumerate(debate_arguments[debate_topic][\"pro\"]):\n",
    "        point_argument = {\n",
    "            'argument': pro_argument['point'],\n",
    "            'pair_id': str(i),\n",
    "            'type': 'point',\n",
    "            'stance': 'PRO'\n",
    "        }\n",
    "        debate_arguments_df = pd.concat([debate_arguments_df, pd.DataFrame([point_argument])], axis=0)\n",
    "        debate_arguments_df = debate_arguments_df.reset_index(drop=True)\n",
    "\n",
    "        if 'counter' in pro_argument.keys():\n",
    "            counter_argument = {\n",
    "                'argument': pro_argument['counter'],\n",
    "                'pair_id': str(i),\n",
    "                'type': 'counter',\n",
    "                'stance': 'CON'\n",
    "            }\n",
    "            debate_arguments_df = pd.concat([debate_arguments_df, pd.DataFrame([counter_argument])], axis=0)\n",
    "            debate_arguments_df = debate_arguments_df.reset_index(drop=True)\n",
    "    \n",
    "    offset = len(debate_arguments[debate_topic][\"pro\"])\n",
    "    \n",
    "    # loop through all argument pairs in the # CON section\n",
    "    for j, con_argument in enumerate(debate_arguments[debate_topic][\"con\"]):\n",
    "        point_argument = {\n",
    "            'argument': con_argument['point'],\n",
    "            'pair_id': str(j+offset),\n",
    "            'type': 'point',\n",
    "            'stance': 'CON'\n",
    "        }\n",
    "        debate_arguments_df = pd.concat([debate_arguments_df, pd.DataFrame([point_argument])], axis=0)\n",
    "        debate_arguments_df = debate_arguments_df.reset_index(drop=True)\n",
    "\n",
    "        if 'counter' in con_argument.keys():\n",
    "            counter_argument = {\n",
    "                'argument': con_argument['counter'],\n",
    "                'pair_id': str(j+offset),\n",
    "                'type': 'counter',\n",
    "                'stance': 'PRO'\n",
    "            }\n",
    "            debate_arguments_df = pd.concat([debate_arguments_df, pd.DataFrame([counter_argument])], axis=0)\n",
    "            debate_arguments_df = debate_arguments_df.reset_index(drop=True)\n",
    "            \n",
    "    debate_arguments_df['topic'] = debate_topic\n",
    "    debate_arguments_df = debate_arguments_df.dropna()\n",
    "    _arguments_df_write_to_file(debate_arguments_df, category, debate_topic)\n",
    "    return debate_arguments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Category] Arguments df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Convert category arguments dict into df \"\"\"\n",
    "\n",
    "def category_convert_to_df(category_arguments: {}) -> pd.DataFrame:\n",
    "    category_arguments_df = pd.DataFrame()\n",
    "    category = next(iter(category_arguments))\n",
    "\n",
    "    # Loop through debates in category\n",
    "    debates = category_arguments[category]\n",
    "    for debate in debates:\n",
    "        debate_dict = category_arguments[category][debate]\n",
    "        debate_df = debate_convert_to_df({debate: debate_dict}, category)\n",
    "        category_arguments_df = pd.concat([category_arguments_df, debate_df], axis = 0)\n",
    "        category_arguments_df = category_arguments_df.reset_index(drop=True)\n",
    "\n",
    "    category_arguments_df['category'] = category\n",
    "    category_arguments_df = category_arguments_df.dropna()\n",
    "    _arguments_df_write_to_file(category_arguments_df, category)\n",
    "    return category_arguments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Global] Arguments df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Convert global arguments dict into df \"\"\"\n",
    "\n",
    "def global_convert_to_df(global_arguments: {}) -> pd.DataFrame:\n",
    "    global_arguments_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through categories in global arguments\n",
    "    for category in global_arguments.keys():\n",
    "        global_arguments_df = pd.concat([global_arguments_df, category_convert_to_df({category: global_arguments[category]})], axis=0)\n",
    "        global_arguments_df = global_arguments_df.reset_index(drop=True)\n",
    "\n",
    "    global_arguments_df = global_arguments_df.dropna()\n",
    "    _arguments_df_write_to_file(global_arguments_df)\n",
    "    return global_arguments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Imports] Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Write] Embeddings df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Write extracted embeddings to pickle file \"\"\"\n",
    "\n",
    "def _embeddings_write_to_file(\n",
    "    embeddings_data: pd.DataFrame,\n",
    "    category: Optional[str] = None,\n",
    "    topic: Optional[str] = None\n",
    "    ):\n",
    "\n",
    "    # Debate case\n",
    "    if topic and category:\n",
    "        topic_path = topic.replace('-', '_')\n",
    "        output_folder = f'../data_dump/embeddings_dump/{category}/'\n",
    "        output_file_path = f'{output_folder}{topic_path}_embeddings.pkl'\n",
    "    \n",
    "    # Category case\n",
    "    elif category:\n",
    "        output_folder = f'../data_dump/embeddings_dump/{category}/'\n",
    "        output_file_path = f'{output_folder}{category}_embeddings.pkl'\n",
    "    \n",
    "    # Global case\n",
    "    else:\n",
    "        output_folder = f'../data_dump/embeddings_dump/'\n",
    "        output_file_path = f'{output_folder}global_embeddings.pkl'\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    embeddings_data.to_pickle(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [All] Embeddings df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Convert an argument into a (1 x 1536) embedding df \"\"\"\n",
    "\n",
    "DIM_EMBEDDING = 1536\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=60, max=500), stop=stop_after_attempt(10))\n",
    "def _get_embeddings(arguments: []) -> []:\n",
    "    embeddings = client.embeddings.create(input=arguments, model=\"text-embedding-ada-002\")\n",
    "    embeddings_data = [embedding_data.embedding for embedding_data in embeddings.data]\n",
    "    embeddings_df = pd.DataFrame(embeddings_data, columns=[f\"{str(i)}\" for i in range(DIM_EMBEDDING)])\n",
    "    return embeddings_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Add embeddings column to a df \"\"\"\n",
    "\n",
    "API_LIMIT = 1000\n",
    "\n",
    "def get_embeddings_df(arguments_df: pd.DataFrame, processing_unit: ProcessingUnit, debate_category: Optional[Category] = None) -> pd.DataFrame:\n",
    "    embeddings_df = pd.DataFrame()\n",
    "    arguments_list = list(arguments_df['argument'])\n",
    "    total_len = len(arguments_list)\n",
    "    i = 0\n",
    "\n",
    "    # Grab embeddings from arguments column in chunks\n",
    "    while i < total_len:\n",
    "        embeddings = _get_embeddings(arguments_list[i:min(total_len, i+API_LIMIT)])\n",
    "        embeddings_df = pd.concat([embeddings_df, embeddings], axis=0, ignore_index=True)\n",
    "        i = i + API_LIMIT\n",
    "    arguments_embeddings_df = pd.concat([arguments_df, embeddings_df], axis=1)\n",
    "    \n",
    "    # Write embeddings df to file\n",
    "    if processing_unit == ProcessingUnit.GLOBAL:\n",
    "        _embeddings_write_to_file(arguments_embeddings_df, None, None)\n",
    "    elif processing_unit == ProcessingUnit.CATEGORY:\n",
    "        _embeddings_write_to_file(arguments_embeddings_df, arguments_embeddings_df['category'].iloc[0], None)\n",
    "    elif processing_unit == ProcessingUnit.DEBATE:\n",
    "        _embeddings_write_to_file(arguments_embeddings_df, debate_category.value, arguments_embeddings_df['topic'].iloc[0])\n",
    "    return arguments_embeddings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Imports] Analysis df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Write] Analysis df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Write analysis results to pickle file \"\"\"\n",
    "\n",
    "def _analysis_write_to_file(\n",
    "    analysis_type: AnalysisType,\n",
    "    processing_unit: ProcessingUnit, # Does the df contain 1 debate / 1 category / global\n",
    "    analysis_data: pd.DataFrame,\n",
    "    category: Optional[str] = None,\n",
    "    topic: Optional[str] = None\n",
    "    ):\n",
    "    processing_level = processing_unit.value\n",
    "    \n",
    "    # Debate facet\n",
    "    if topic and category:\n",
    "        topic_path = topic.replace('-', '_')\n",
    "        if processing_unit == ProcessingUnit.DEBATE:\n",
    "            output_folder = f'../data_dump/{analysis_type.value}_dump/{category}/debates/'\n",
    "        elif processing_unit == ProcessingUnit.CATEGORY:\n",
    "            output_folder = f'../data_dump/{analysis_type.value}_dump/{category}/category-facet-debates/'\n",
    "        elif processing_unit == ProcessingUnit.GLOBAL:\n",
    "            output_folder = f'../data_dump/{analysis_type.value}_dump/{category}/global-facet-debates/'\n",
    "        output_file_path = f'{output_folder}{topic_path}_{analysis_type.value}.pkl'\n",
    "    \n",
    "    # Category facet\n",
    "    elif category:\n",
    "        output_folder = f'../data_dump/{analysis_type.value}_dump/{category}/'\n",
    "        if processing_unit == ProcessingUnit.CATEGORY:\n",
    "            output_file_path = f'{output_folder}{category}_{analysis_type.value}.pkl'\n",
    "        elif processing_unit == ProcessingUnit.GLOBAL:\n",
    "            output_file_path = f'{output_folder}global_category_facet_{category}_{analysis_type.value}.pkl'\n",
    "            \n",
    "    # Global facet\n",
    "    else:\n",
    "        if processing_unit == ProcessingUnit.GLOBAL:\n",
    "            output_folder = f'../data_dump/{analysis_type.value}_dump/'\n",
    "            output_file_path = f'{output_folder}global_{analysis_type.value}.pkl'\n",
    "        else:\n",
    "            print(f\"Invalid processing unit: {processing_unit}.\")\n",
    "   \n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    analysis_data.to_pickle(output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Debate] Analysis df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_normalization(pair_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Normalize PCA argument embeddings from a df\n",
    "    \"\"\"\n",
    "    ret_df = pd.DataFrame()\n",
    "    \n",
    "    point_row = pair_df[pair_df['type'] == 'point']\n",
    "    counter_row = pair_df[pair_df['type'] == 'counter']\n",
    "    \n",
    "    point_nonnum_row = point_row.select_dtypes(exclude=[np.number])\n",
    "    counter_nonnum_row = counter_row.select_dtypes(exclude=[np.number])\n",
    "    new_nonnum_rows = pd.concat([point_nonnum_row, counter_nonnum_row])\n",
    "    new_nonnum_rows = new_nonnum_rows.reset_index(drop=True)\n",
    "    \n",
    "    point_vec = point_row.select_dtypes(include=[np.number]).values.flatten()\n",
    "    counter_vec = counter_row.select_dtypes(include=[np.number]).values.flatten()\n",
    "\n",
    "    center = (point_vec.copy() + counter_vec.copy()) / 2\n",
    "    point_vec -= center\n",
    "    counter_vec -= center\n",
    "    point_vec = point_vec.flatten()\n",
    "    counter_vec = counter_vec.flatten()\n",
    "\n",
    "    point_new_num_row = pd.DataFrame(point_vec.reshape(1,-1), columns=['{}'.format(i) for i in range(len(point_vec))])\n",
    "    counter_new_num_row = pd.DataFrame(counter_vec.reshape(1,-1), columns=['{}'.format(i) for i in range(len(counter_vec))])\n",
    "    new_num_rows = pd.concat([point_new_num_row, counter_new_num_row])\n",
    "    new_num_rows = new_num_rows.reset_index(drop=True)\n",
    "    return new_nonnum_rows.join(new_num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_preprocessing(embeddings_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if len(embeddings_df) % 2 != 0:\n",
    "        print(\"Warning: embeddings_df not in pairs\")\n",
    "    embeddings_ret_df = pd.DataFrame()\n",
    "    topics = list(embeddings_df['topic'].unique())\n",
    "\n",
    "    for topic in topics:\n",
    "        topic_rows = embeddings_df[embeddings_df['topic'] == topic]\n",
    "        if len(topic_rows) % 2 != 0:\n",
    "            print(f\"Warning: Topic '{topic}' has {len(topic_rows)} rows.\")\n",
    "        pair_ids = list(topic_rows['pair_id'].unique())\n",
    "        \n",
    "        for pair_id in pair_ids:\n",
    "            pair_df = topic_rows[topic_rows['pair_id'] == pair_id]\n",
    "            if len(pair_df) != 2:\n",
    "                print(f\"Warning: Pair {pair_id} at topic '{topic}' has {len(pair_df)} rows.\")\n",
    "                continue\n",
    "            embeddings_ret_df = pd.concat([embeddings_ret_df, pca_normalization(pair_df)])\n",
    "            embeddings_ret_df = embeddings_ret_df.reset_index(drop=True)\n",
    "    return embeddings_ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_embeddings(\n",
    "        embeddings_df: pd.DataFrame,\n",
    "        num_components: int,\n",
    "        processing_unit: ProcessingUnit=None,\n",
    "        facet: ProcessingUnit=None,\n",
    "        debate_category: Optional[str] = None\n",
    "    ):\n",
    "    embeddings_processed = pca_preprocessing(embeddings_df)\n",
    "    numeric_columns = embeddings_processed.select_dtypes(include=[np.number]).columns\n",
    "    non_numeric_columns = embeddings_processed.select_dtypes(exclude=[np.number]).columns\n",
    "    embeddings_data = embeddings_processed[numeric_columns].values\n",
    "    \n",
    "    # scaler = StandardScaler()\n",
    "    # embedding_vectors_scaled = scaler.fit_transform(embeddings_data)\n",
    "    num_components = min(num_components, embeddings_data.shape[0], embeddings_data.shape[1])\n",
    "    pca = PCA(n_components=num_components)\n",
    "    embeddings_pca = pca.fit_transform(embeddings_data)\n",
    "    embeddings_pca_data = (\n",
    "        pd.DataFrame(embeddings_pca, columns=['pca_{}'.format(i) for i in range(num_components)])\n",
    "        .join(embeddings_processed[non_numeric_columns].reset_index(drop=True))\n",
    "    )\n",
    "\n",
    "    # Printing PCA attributes\n",
    "    components = pca.components_\n",
    "    explained_variance = pca.explained_variance_\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    singular_values = pca.singular_values_\n",
    "    mean_value = pca.mean_\n",
    "    n_components = pca.n_components_\n",
    "    n_features = pca.n_features_in_\n",
    "    n_samples = pca.n_samples_\n",
    "    \n",
    "    # Displaying the attributes\n",
    "    print(\"Components:\")\n",
    "    print(components)\n",
    "    print(\"\\nExplained Variance:\")\n",
    "    print(explained_variance)\n",
    "    print(\"\\nExplained Variance Ratio:\")\n",
    "    print(explained_variance_ratio)\n",
    "    print(\"\\nSingular Values:\")\n",
    "    print(singular_values)\n",
    "    print(\"\\nMean:\")\n",
    "    print(mean_value)\n",
    "    print(\"\\nNumber of Components:\")\n",
    "    print(n_components)\n",
    "    print(\"\\nNumber of Features:\")\n",
    "    print(n_features)\n",
    "    print(\"\\nNumber of Samples:\")\n",
    "    print(n_samples)\n",
    "\n",
    "    # Write to file\n",
    "    if facet == ProcessingUnit.DEBATE:\n",
    "        _analysis_write_to_file(AnalysisType.PCA, processing_unit, embeddings_pca_data, debate_category, embeddings_pca_data['topic'].iloc[0])\n",
    "    elif facet == ProcessingUnit.CATEGORY:\n",
    "        _analysis_write_to_file(AnalysisType.PCA, processing_unit, embeddings_pca_data, embeddings_pca_data['category'].iloc[0])\n",
    "    elif facet == ProcessingUnit.GLOBAL:\n",
    "        _analysis_write_to_file(AnalysisType.PCA, processing_unit, embeddings_pca_data)\n",
    "    else:\n",
    "        pass\n",
    "    return embeddings_pca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debate_analyze_embeddings(\n",
    "        analysis_type: AnalysisType,\n",
    "        num_components,\n",
    "        debate_embeddings_df: pd.DataFrame,\n",
    "        debate_category: str\n",
    "    ):\n",
    "    \"\"\" Analyze argument embeddings from a debate df\n",
    "    \"\"\"\n",
    "    if analysis_type == AnalysisType.TSNE:\n",
    "        debate_embeddings_analysis = tsne_embeddings(debate_embeddings_df, ProcessingUnit.DEBATE, ProcessingUnit.DEBATE, debate_category)\n",
    "    elif analysis_type == AnalysisType.PCA:\n",
    "        debate_embeddings_analysis = pca_embeddings(debate_embeddings_df, num_components, ProcessingUnit.DEBATE, ProcessingUnit.DEBATE, debate_category)\n",
    "    return debate_embeddings_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Category] Analysis df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_analyze_embeddings(\n",
    "        analysis_type: AnalysisType,\n",
    "        num_components,\n",
    "        category_embeddings_df: pd.DataFrame,\n",
    "        facet: ProcessingUnit\n",
    "    ):\n",
    "    \"\"\" Analyze argument embeddings from a category df\n",
    "    \"\"\"\n",
    "    # Analyze Embeddings\n",
    "    category = category_embeddings_df['category'].iloc[0]\n",
    "    if analysis_type == AnalysisType.TSNE:\n",
    "        if facet == ProcessingUnit.CATEGORY:\n",
    "            category_embeddings_analysis = tsne_embeddings(category_embeddings_df, ProcessingUnit.CATEGORY, facet)\n",
    "        elif facet == ProcessingUnit.DEBATE:\n",
    "            category_embeddings_analysis = category_embeddings_df.groupby('topic').apply(lambda group: tsne_embeddings(group, ProcessingUnit.CATEGORY, facet, category))\n",
    "            category_embeddings_analysis = category_embeddings_analysis.reset_index(drop=True)\n",
    "        else:\n",
    "            print(f\"Inappropriate facet level: {facet}.\")\n",
    "    elif analysis_type == AnalysisType.PCA:\n",
    "        if facet == ProcessingUnit.CATEGORY:\n",
    "            category_embeddings_analysis = pca_embeddings(category_embeddings_df, num_components, ProcessingUnit.CATEGORY, facet)\n",
    "        elif facet == ProcessingUnit.DEBATE:\n",
    "            category_embeddings_analysis = category_embeddings_df.groupby('topic').apply(lambda group: pca_embeddings(group, num_components, ProcessingUnit.CATEGORY, facet, category))\n",
    "            category_embeddings_analysis = category_embeddings_analysis.reset_index(drop=True)\n",
    "        else:\n",
    "            print(f\"Inappropriate facet level: {facet}.\")\n",
    "    \n",
    "    # Write to file for facet\n",
    "    if facet == ProcessingUnit.DEBATE:\n",
    "        output_folder = f'../data_dump/{analysis_type.value}_dump/{category}/'\n",
    "        output_file_path = f'{output_folder}category_debate_facet_{category}_{analysis_type.value}.pkl'\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        category_embeddings_analysis.to_pickle(output_file_path)\n",
    "    return category_embeddings_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Global] Analysis df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_analyze_embeddings(\n",
    "    analysis_type: AnalysisType,\n",
    "    num_components: int,\n",
    "    global_embeddings_df: pd.DataFrame,\n",
    "    facet: ProcessingUnit\n",
    "    ):\n",
    "    \"\"\" Analyze argument embeddings from a global df\n",
    "    \"\"\"\n",
    "    if analysis_type == AnalysisType.TSNE:\n",
    "        if facet == ProcessingUnit.GLOBAL:\n",
    "            global_embeddings_analysis = tsne_embeddings(global_embeddings_df, ProcessingUnit.GLOBAL, facet)\n",
    "        elif facet == ProcessingUnit.CATEGORY:\n",
    "            global_embeddings_analysis = global_embeddings_df.groupby('category').apply(lambda group: tsne_embeddings(group, ProcessingUnit.GLOBAL, facet))\n",
    "        elif facet == ProcessingUnit.DEBATE:\n",
    "            global_embeddings_analysis = global_embeddings_df.groupby('topic').apply(lambda group: tsne_embeddings(group, ProcessingUnit.GLOBAL, facet, group['category'].iloc[0]))\n",
    "    elif analysis_type == AnalysisType.PCA:\n",
    "        if facet == ProcessingUnit.GLOBAL:\n",
    "            global_embeddings_analysis = pca_embeddings(global_embeddings_df, num_components, ProcessingUnit.GLOBAL, facet)\n",
    "        elif facet == ProcessingUnit.CATEGORY:\n",
    "            global_embeddings_analysis = global_embeddings_df.groupby('category').apply(lambda group: pca_embeddings(group, num_components, ProcessingUnit.GLOBAL, facet))\n",
    "        elif facet == ProcessingUnit.DEBATE:\n",
    "            global_embeddings_analysis = global_embeddings_df.groupby('topic').apply(lambda group: pca_embeddings(group, num_components, ProcessingUnit.GLOBAL, facet, group['category'].iloc[0]))\n",
    "    \n",
    "    # Write to file for facet\n",
    "    if facet != ProcessingUnit.GLOBAL:\n",
    "        output_folder = f'../data_dump/{analysis_type.value}_dump/'\n",
    "        output_file_path = f'{output_folder}global_{facet.value}_facet_{analysis_type.value}.pkl'\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "        global_embeddings_analysis.to_pickle(output_file_path)\n",
    "    return global_embeddings_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Imports] Analysis Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import ggplot, geom_point, geom_text, geom_line, aes, theme, theme_void, labs, element_text, facet_wrap, ggsave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Debate] Analysis Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _insert_line_breaks(text, max_width=50):\n",
    "    words = text.split(' ')\n",
    "    lines = []\n",
    "    current_line = ''\n",
    "\n",
    "    for word in words:\n",
    "        if len(current_line) + len(word) <= max_width:\n",
    "            current_line += word + ' '\n",
    "        else:\n",
    "            lines.append(current_line.strip())\n",
    "            current_line = word + ' '\n",
    "\n",
    "    lines.append(current_line.strip())\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debate_plot(\n",
    "        analysis_type: AnalysisType,\n",
    "        debate_category: str,\n",
    "        embeddings_analysis_data: pd.DataFrame,\n",
    "        processing_unit: ProcessingUnit=ProcessingUnit.DEBATE\n",
    "    ):\n",
    "    \"\"\" Plot embeddings for a single debate\n",
    "    \"\"\"\n",
    "    # Plot\n",
    "    stance_markers = {'PRO': '+', 'CON': '*'}\n",
    "    debate_topic = embeddings_analysis_data['topic'].iloc[0]\n",
    "    plot_topic = _insert_line_breaks(debate_topic.replace('-', ' '))\n",
    "    plot_analysis_type = analysis_type.value.upper()\n",
    "    gg = (\n",
    "        ggplot(embeddings_analysis_data, aes(x='x', y='y', color='stance', shape='stance', group='pair_id')) +\n",
    "        geom_point(size=2) +\n",
    "        geom_line(color='black', size=0.5) +\n",
    "        labs(\n",
    "            title=f'{plot_analysis_type} Plot for Debate:\\n{plot_topic}',\n",
    "            x=f'{plot_analysis_type}_x',\n",
    "            y=f'{plot_analysis_type}_y'\n",
    "        ) +\n",
    "        theme(\n",
    "            axis_title=element_text(margin={'t': 20}),\n",
    "            figure_size=(8, 8),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Save to file\n",
    "    if processing_unit == ProcessingUnit.DEBATE:\n",
    "        output_folder = f'../data_dump/{analysis_type.value}_plots_dump/{debate_category}/debate-plots/'\n",
    "    elif processing_unit == ProcessingUnit.CATEGORY:\n",
    "        output_folder = f'../data_dump/{analysis_type.value}_plots_dump/{debate_category}/debate-plots/category-facet-debate-plots/'\n",
    "    elif processing_unit == ProcessingUnit.GLOBAL:\n",
    "        output_folder = f'../data_dump/{analysis_type.value}_plots_dump/{debate_category}/debate-plots/global-facet-debate-plots/'\n",
    "    output_file_path = f'{output_folder}{debate_topic}_{analysis_type.value}_plot.png'\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    ggsave(gg, output_file_path)\n",
    "    print(gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Category] Analysis Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plot embeddings for debates in a category \"\"\"\n",
    "\n",
    "def category_plot(\n",
    "        analysis_type: AnalysisType,\n",
    "        category_plot_data: pd.DataFrame,\n",
    "        processing_unit: ProcessingUnit=ProcessingUnit.CATEGORY,\n",
    "        facet: ProcessingUnit=ProcessingUnit.CATEGORY,\n",
    "        view: ProcessingUnit=ProcessingUnit.CATEGORY\n",
    "    ):\n",
    "    \n",
    "    # Plot\n",
    "    plot_category = category_plot_data['category'].iloc[0]\n",
    "    plot_analysis_type = analysis_type.value.upper()\n",
    "    category_plot_data['interaction'] = category_plot_data['pair_id'] + '_' + category_plot_data['topic']\n",
    "    if view == ProcessingUnit.CATEGORY:\n",
    "        gg = (\n",
    "            ggplot(category_plot_data, aes(x='x', y='y', color='topic', shape='stance', group='interaction')) +\n",
    "            geom_point(size=2) +\n",
    "            geom_line(color='black', size=0.5) +\n",
    "            labs(\n",
    "                title=f'{plot_analysis_type} Plot for Category:\\n{plot_category}',\n",
    "                x=f'{plot_analysis_type}_x',\n",
    "                y=f'{plot_analysis_type}_y'\n",
    "            ) +\n",
    "            theme(\n",
    "                legend_position=\"none\",\n",
    "                plot_title=element_text(size=24),\n",
    "                strip_text=element_text(angle=0, hjust=0.5, vjust=1, wrap=True),\n",
    "                figure_size=(16, 16)\n",
    "            )\n",
    "        )\n",
    "    elif view == ProcessingUnit.DEBATE:\n",
    "        gg = (\n",
    "            ggplot(category_plot_data, aes(x='x', y='y', group='interaction')) +\n",
    "            facet_wrap('~topic', ncol=5, scales='free') +\n",
    "            geom_point(aes(color='stance'), size=1) +\n",
    "            geom_line(color='black', size=0.5) +\n",
    "            labs(\n",
    "                title=f'{plot_analysis_type} Plot for Category:\\n{plot_category}',\n",
    "                x=f'{plot_analysis_type}_x',\n",
    "                y=f'{plot_analysis_type}_y'\n",
    "            ) +\n",
    "            theme(\n",
    "                axis_title=element_text(size=16),\n",
    "                plot_title=element_text(size=32),\n",
    "                strip_text=element_text(angle=0, hjust=0.5, vjust=1, wrap=True),\n",
    "                figure_size=(24, 24)\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(f'Inappropriate view level: {facet}')\n",
    "        \n",
    "    # Save to file\n",
    "    output_folder = f'../data_dump/{analysis_type.value}_plots_dump/{plot_category}/{view.value}-view/'\n",
    "    if processing_unit == ProcessingUnit.CATEGORY:\n",
    "        if facet == ProcessingUnit.CATEGORY:\n",
    "            output_file_path = f'{output_folder}{plot_category}_{analysis_type.value}_plot.png'\n",
    "        elif facet == ProcessingUnit.DEBATE:\n",
    "            output_file_path = f'{output_folder}category_debate_facet_{plot_category}_{analysis_type.value}_plot.png'\n",
    "    elif processing_unit == ProcessingUnit.GLOBAL:\n",
    "        output_file_path = f'{output_folder}global_category_facet_{plot_category}_{analysis_type.value}_plot.png'\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    ggsave(gg, output_file_path)\n",
    "    print(gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Global] Analysis Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot embeddings for all debates\n",
    "def global_plot(\n",
    "        analysis_type: AnalysisType,\n",
    "        global_plot_data: pd.DataFrame,\n",
    "        facet: ProcessingUnit=ProcessingUnit.GLOBAL,\n",
    "        view: ProcessingUnit=ProcessingUnit.GLOBAL\n",
    "    ):\n",
    "\n",
    "    # Plot\n",
    "    plot_analysis_type = analysis_type.value.upper()\n",
    "    global_plot_data['interaction'] = global_plot_data['pair_id'] + '_' + global_plot_data['topic']\n",
    "    if view == ProcessingUnit.GLOBAL:\n",
    "        gg = (\n",
    "            ggplot(global_plot_data, aes(x='x', y='y', color='category', shape='stance', group='interaction')) +\n",
    "            geom_point(size=2) +\n",
    "            geom_line(color='black', size=0.5) +\n",
    "            labs(\n",
    "                title=f'{plot_analysis_type} Plot for All Debates',\n",
    "                x=f'{plot_analysis_type}_x',\n",
    "                y=f'{plot_analysis_type}_y'\n",
    "            ) +\n",
    "            theme(\n",
    "                legend_position=\"none\",\n",
    "                plot_title=element_text(size=32),\n",
    "                strip_text=element_text(angle=0, hjust=0.5, vjust=1, wrap=True),\n",
    "                figure_size=(24, 24)\n",
    "            )\n",
    "        )\n",
    "    elif view == ProcessingUnit.CATEGORY:\n",
    "        gg = (\n",
    "            ggplot(global_plot_data.reset_index(drop=True), aes(x='x', y='y', group='interaction')) +\n",
    "            facet_wrap('~category', ncol=2, scales='free') +\n",
    "            geom_point(aes(color='topic'), size=1) +\n",
    "            geom_line(color='black', size=0.5) +\n",
    "            labs(\n",
    "                title=f'{plot_analysis_type} Plot for All Categories',\n",
    "                x=f'{plot_analysis_type}_x',\n",
    "                y=f'{plot_analysis_type}_y'\n",
    "            ) +\n",
    "            theme(\n",
    "                axis_title=element_text(size=16),\n",
    "                plot_title=element_text(size=32),\n",
    "                strip_text=element_text(angle=0, hjust=0.5, vjust=1, wrap=True),\n",
    "                legend_position='none',\n",
    "                figure_size=(24, 24)\n",
    "            )\n",
    "        )\n",
    "    elif view == ProcessingUnit.DEBATE:\n",
    "        print('But why? This is very very not recommended.')\n",
    "        gg = (\n",
    "            ggplot(global_plot_data.reset_index(drop=True), aes(x='x', y='y', group='interaction')) +\n",
    "            facet_wrap('~category', scales='free') +\n",
    "            facet_wrap('~topic', ncol=5, scales='free') +\n",
    "            geom_point(aes(color='stance'), size=1) +\n",
    "            geom_line(color='black', size=0.5) +\n",
    "            labs(\n",
    "                title=f'{plot_analysis_type} Plot for All Debates Across All Categories',\n",
    "                x=f'{plot_analysis_type}_x',\n",
    "                y=f'{plot_analysis_type}_y'\n",
    "            ) +\n",
    "            theme(\n",
    "                axis_title=element_text(size=16),\n",
    "                plot_title=element_text(size=32),\n",
    "                strip_text=element_text(angle=0, hjust=0.5, vjust=1, wrap=True),\n",
    "                legend_position='none',\n",
    "                figure_size=(24, 24)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Save to file\n",
    "    output_folder = f'../data_dump/{analysis_type.value}_plots_dump/{view.value}-view/'\n",
    "    if facet == ProcessingUnit.GLOBAL:\n",
    "        output_file_path = f'{output_folder}global_{analysis_type.value}_plot.png'\n",
    "    elif facet == ProcessingUnit.CATEGORY:\n",
    "        output_file_path = f'{output_folder}global_category_facet_{analysis_type.value}_plot.png'\n",
    "    elif facet == ProcessingUnit.DEBATE:\n",
    "        output_file_path = f'{output_folder}global_debate_facet_{analysis_type.value}_plot.png'\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    ggsave(gg, output_file_path)\n",
    "    print(gg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Run] Extract Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run debate level extract arguments\n",
    "economy_debate_arguments = debate_extract_arguments(Category.ECONOMY, \"business-economy-general-house-would-prohibit-retailers-selling-certain-items\")\n",
    "economy_debate_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run category level extract arguments\n",
    "economy_category_arguments = category_extract_arguments(Category.ECONOMY)\n",
    "economy_category_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run global level extract arguments\n",
    "global_arguments = global_extract_arguments()\n",
    "global_arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Run] Convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run debate level convert to dataframe\n",
    "economy_debate_arguments_df = debate_convert_to_df(economy_debate_arguments, Category.ECONOMY.value)\n",
    "economy_debate_arguments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run category level convert to dataframe\n",
    "economy_category_arguments_df = category_convert_to_df(economy_category_arguments)\n",
    "economy_category_arguments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run global level convert to dataframe\n",
    "global_arguments_df = global_convert_to_df(global_arguments)\n",
    "global_arguments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Load] Arguments df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments_data_path = \"../data_dump/arguments_dump/\"\n",
    "loaded_economy_debate_arguments_df = pd.read_pickle(f\"{arguments_data_path}economy/business_economy_general_house_would_prohibit_retailers_selling_certain_items_arguments.pkl\")\n",
    "loaded_economy_category_arguments_df = pd.read_pickle(f\"{arguments_data_path}economy/economy_arguments.pkl\")\n",
    "loaded_global_arguments_df = pd.read_pickle(f\"{arguments_data_path}global_arguments.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Run] Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run debate level get embeddings\n",
    "economy_debate_embeddings_df = get_embeddings_df(loaded_economy_debate_arguments_df, ProcessingUnit.DEBATE, Category.ECONOMY)\n",
    "economy_debate_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run category level get embeddings\n",
    "loaded_economy_category_arguments_df = pd.read_pickle(f\"{arguments_data_path}economy/economy_arguments.pkl\")\n",
    "economy_category_embeddings_df = get_embeddings_df(loaded_economy_category_arguments_df, ProcessingUnit.CATEGORY)\n",
    "economy_category_embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run global level get embeddings\n",
    "global_embeddings_df = get_embeddings_df(loaded_global_arguments_df, ProcessingUnit.GLOBAL)\n",
    "global_embeddings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Load] Embeddings df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_data_path = \"../data_dump/embeddings_dump/\"\n",
    "loaded_economy_debate_embeddings_df = pd.read_pickle(f\"{embeddings_data_path}economy/business_economy_general_house_would_prohibit_retailers_selling_certain_items_embeddings.pkl\")\n",
    "loaded_economy_category_embeddings_df = pd.read_pickle(f\"{embeddings_data_path}economy/economy_embeddings.pkl\")\n",
    "loaded_global_embeddings_df = pd.read_pickle(f\"{embeddings_data_path}global_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_global_embeddings_df2 = loaded_global_embeddings_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pair_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix = pd.DataFrame()\n",
    "for group_key, rows in loaded_global_embeddings_df2:\n",
    "    matrix = pd.concat([matrix, rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = pca_preprocessing(loaded_global_embeddings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_global_embeddings_df2.groups.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Run]  Analyze Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run debate level pca embeddings\n",
    "economy_debate_pca_embeddings = debate_analyze_embeddings(AnalysisType.PCA, 10, loaded_economy_debate_embeddings_df, \"economy\")\n",
    "economy_debate_pca_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run category level category facet pca embeddings\n",
    "economy_category_pca_category_facet_embeddings = category_analyze_embeddings(AnalysisType.PCA, 10, loaded_economy_category_embeddings_df, ProcessingUnit.CATEGORY)\n",
    "economy_category_pca_category_facet_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run category level debate facet pca embeddings\n",
    "economy_category_pca_debate_facet_embeddings = category_analyze_embeddings(AnalysisType.PCA, 10, loaded_economy_category_embeddings_df, ProcessingUnit.DEBATE)\n",
    "economy_category_pca_debate_facet_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run global level global facet pca embeddings\n",
    "global_pca_global_facet_embeddings = global_analyze_embeddings(AnalysisType.PCA, 10, loaded_global_embeddings_df, ProcessingUnit.GLOBAL)\n",
    "global_pca_global_facet_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run global level category facet pca embeddings\n",
    "global_pca_category_facet_embeddings = global_analyze_embeddings(AnalysisType.PCA, 10, loaded_global_embeddings_df, ProcessingUnit.CATEGORY)\n",
    "global_pca_category_facet_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run global level debate facet pca embeddings\n",
    "global_pca_debate_facet_embeddings = global_analyze_embeddings(AnalysisType.PCA, 10, loaded_global_embeddings_df, ProcessingUnit.DEBATE)\n",
    "global_pca_debate_facet_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Load] Analysis df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" pca \"\"\"\n",
    "\n",
    "pca_data_path = \"../data_dump/pca_dump/\"\n",
    "\n",
    "# Debate level\n",
    "loaded_economy_debate_pca_df = pd.read_pickle(f\"{pca_data_path}economy/debates/business_economy_general_house_would_prohibit_retailers_selling_certain_items_pca.pkl\")\n",
    "loaded_category_debate_facet_economy_debate_pca_df = pd.read_pickle(f\"{pca_data_path}economy/category-facet-debates/business_economy_general_house_would_prohibit_retailers_selling_certain_items_pca.pkl\")\n",
    "loaded_global_debate_facet_economy_debate_pca_df = pd.read_pickle(f\"{pca_data_path}economy/global-facet-debates/business_economy_general_house_would_prohibit_retailers_selling_certain_items_pca.pkl\")\n",
    "\n",
    "# Category level\n",
    "loaded_economy_category_pca_df = pd.read_pickle(f\"{pca_data_path}economy/economy_pca.pkl\")\n",
    "loaded_category_debate_facet_economy_category_pca_df = pd.read_pickle(f\"{pca_data_path}economy/category_debate_facet_economy_pca.pkl\")\n",
    "loaded_global_category_facet_economy_category_pca_df = pd.read_pickle(f\"{pca_data_path}economy/global_category_facet_economy_pca.pkl\")\n",
    "\n",
    "# Global level\n",
    "loaded_global_pca_df = pd.read_pickle(f\"{pca_data_path}global_pca.pkl\")\n",
    "loaded_global_category_facet_pca_df = pd.read_pickle(f\"{pca_data_path}global_category_facet_pca.pkl\")\n",
    "loaded_global_debate_facet_pca_df = pd.read_pickle(f\"{pca_data_path}global_debate_facet_pca.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_global_debate_facet_pca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Run] Debate Level PCA Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run debate level pca plot\n",
    "debate_plot(AnalysisType.PCA, \"economy\", loaded_economy_debate_pca_df, ProcessingUnit.DEBATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run debate level category facet pca plot\n",
    "debate_plot(AnalysisType.PCA, \"economy\", loaded_category_debate_facet_economy_debate_pca_df, ProcessingUnit.CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run debate level global facet pca plot\n",
    "debate_plot(AnalysisType.PCA, \"economy\", loaded_global_debate_facet_economy_debate_pca_df, ProcessingUnit.GLOBAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Run] Category Level PCA Plots -> Category View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run category level category view pca plot\n",
    "category_plot(AnalysisType.PCA, loaded_economy_category_pca_df, ProcessingUnit.CATEGORY, ProcessingUnit.CATEGORY, ProcessingUnit.CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run category level category debate facet category view pca plot\n",
    "category_plot(AnalysisType.PCA, loaded_category_debate_facet_economy_category_pca_df, ProcessingUnit.CATEGORY, ProcessingUnit.DEBATE, ProcessingUnit.CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run category level global category facet category view pca plot\n",
    "category_plot(AnalysisType.PCA, loaded_global_category_facet_economy_category_pca_df, ProcessingUnit.GLOBAL, ProcessingUnit.CATEGORY, ProcessingUnit.CATEGORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Run] Category Level PCA Plots -> Debate View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run category level debates view pca plot\n",
    "category_plot(AnalysisType.PCA, loaded_economy_category_pca_df, ProcessingUnit.CATEGORY, ProcessingUnit.CATEGORY, ProcessingUnit.DEBATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run category level category debate facet debates view pca plot\n",
    "category_plot(AnalysisType.PCA, loaded_category_debate_facet_economy_category_pca_df, ProcessingUnit.CATEGORY, ProcessingUnit.DEBATE, ProcessingUnit.DEBATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run category level global category facet debates view pca plot\n",
    "category_plot(AnalysisType.PCA, loaded_global_category_facet_economy_category_pca_df, ProcessingUnit.GLOBAL, ProcessingUnit.CATEGORY, ProcessingUnit.DEBATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Run] Global Level PCA Plots -> Global View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run global level global view pca plot\n",
    "global_plot(AnalysisType.PCA, loaded_global_pca_df, ProcessingUnit.GLOBAL, ProcessingUnit.GLOBAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run global level global category facet global view pca plot\n",
    "global_plot(AnalysisType.PCA, loaded_global_category_facet_pca_df, ProcessingUnit.CATEGORY, ProcessingUnit.GLOBAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run global level global debate facet global view pca plot\n",
    "global_plot(AnalysisType.PCA, loaded_global_debate_facet_pca_df, ProcessingUnit.DEBATE, ProcessingUnit.GLOBAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### [Run] Global Level PCA Plots -> Category View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run global level category view pca plot\n",
    "global_plot(AnalysisType.PCA, loaded_global_pca_df, ProcessingUnit.GLOBAL, ProcessingUnit.CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run global level global category facet category view pca plot\n",
    "global_plot(AnalysisType.PCA, loaded_global_category_facet_pca_df, ProcessingUnit.CATEGORY, ProcessingUnit.CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run global level global debate facet category view pca plot\n",
    "global_plot(AnalysisType.PCA, loaded_global_debate_facet_pca_df, ProcessingUnit.DEBATE, ProcessingUnit.CATEGORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert file path "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
